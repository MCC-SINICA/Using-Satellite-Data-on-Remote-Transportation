{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import threading\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from collections import Iterable\n",
    "from tensorflow.keras import losses\n",
    "from statistics import mean\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint, EarlyStopping,Callback\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers import *\n",
    "#K.set_image_data_format('channels_first')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "n_channels = 1\n",
    "n_classes = 18\n",
    "sample_batch = 10\n",
    "time_step = 1\n",
    "time_step2 = 3\n",
    "number_stations = 18\n",
    "#optim = Adam(lr = 0.0000, decay = 0.000015)\n",
    "optim = Adam(learning_rate = 1e-4, decay = 1e-4)\n",
    "# Setup the model\n",
    "\n",
    "\n",
    "def eval_metrics_on(predictions,labels):\n",
    "    '''assuming this is a regression task; labels are continuous-valued floats\n",
    "    \n",
    "    returns most regression-related scores for the given predictions/targets as a dictionary:\n",
    "    \n",
    "        r2, mean_abs_error, mse, rmse, median_absolute_error, explained_variance_score\n",
    "    '''\n",
    "    #if len(labels[0])==2: #labels is list of data/labels pairs\n",
    "        #labels = np.concatenate([l[1] for l in labels])\n",
    "    #predictions = predictions[:,0]\n",
    "    prdictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    r2                       = metrics.r2_score(labels, predictions)\n",
    "    mean_abs_error           = np.abs(predictions - labels).mean()\n",
    "    mse                      = ((predictions - labels)**2).mean()\n",
    "    rmse                     = np.sqrt(mse)\n",
    "    #median_absolute_error    = metrics.median_absolute_error(labels, predictions) # robust to outliers\n",
    "    explained_variance_score = metrics.explained_variance_score(labels, predictions) # best score = 1, lower is worse\n",
    "    return {'r2':r2, 'mean_abs_error':mean_abs_error, 'mse':mse, 'rmse':rmse, \n",
    "    'explained_variance_score':explained_variance_score}\n",
    "\n",
    "\n",
    "def predict_graph(true_val,predict_val):\n",
    "    for sn in range(number_stations):\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.figure(sn+1)\n",
    "        plt.title('_next_1day station = ' + str(sn+1))\n",
    "        plt.plot(true_val[:,sn::number_stations], c='g', label= 'gt')\n",
    "        plt.plot(predict_val[:,sn::number_stations], c='r', label= 'pred')\n",
    "        plt.legend(loc='best',prop={'size': 5})\n",
    "        plt.legend(loc='best')\n",
    "        name = str(\"firstgroup_stat_\") + str(sn) + str(\".png\")\n",
    "        plt.show()\n",
    "\n",
    "#This function used to clear the memory         \n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "def gui_generator(tile28v05,tile28v06,tile29v05,tile29v06,remoteweather,pm,y_lst,batch_size,num_channels,\n",
    "                  num_channels_2,num_channels_3,\n",
    "                  time,time_label,cover_hr_in=1):\n",
    "    while True:\n",
    "        x_shape_1 = (batch_size,time_label,300,300,num_channels)\n",
    "        \n",
    "        x_shape_2 = (batch_size,time_label,300,300,num_channels)\n",
    "        \n",
    "        x_shape_3 = (batch_size,time,4,7,num_channels_2)\n",
    "        \n",
    "        y_shape = (batch_size,time,18)\n",
    "        \n",
    "        h28v05 = np.zeros(shape=x_shape_1)\n",
    "        h28v06 = np.zeros(shape=x_shape_1)\n",
    "        \n",
    "        h29v05 = np.zeros(shape=x_shape_2)\n",
    "        h29v06 = np.zeros(shape=x_shape_2)\n",
    "        \n",
    "        h31v06 = np.zeros(shape=x_shape_3)\n",
    "        #h32v06 = np.zeros(shape=x_shape_4)\n",
    "        pm_batch = np.zeros(shape=y_shape)\n",
    "        \n",
    "        \n",
    "        #pm = np.zeros(shape=y_shape)\n",
    "        y_batch = np.zeros(shape=y_shape)\n",
    "        \n",
    "        end = y_lst.shape[0]\n",
    "        #print('this is end ', end)\n",
    "        \n",
    "        \n",
    "        for batch_idx in range(0,end,int(batch_size)):\n",
    "            for seq_idx in range(int(batch_size)):\n",
    "                idx = batch_idx + seq_idx\n",
    "                \n",
    "                h28v05[seq_idx] = tile28v05[idx:idx+cover_hr_in][0]\n",
    "                h28v06[seq_idx] = tile28v06[idx:idx+cover_hr_in][0]\n",
    "                \n",
    "                h29v05[seq_idx] = tile29v05[idx:idx+cover_hr_in][0]\n",
    "                h29v06[seq_idx] = tile29v06[idx:idx+cover_hr_in][0]\n",
    "                \n",
    "                h31v06[seq_idx] = remoteweather[idx:idx+cover_hr_in][0]\n",
    "                pm_batch[seq_idx] = pm[idx:idx+cover_hr_in][0]\n",
    "                \n",
    "                y_batch[seq_idx] = y_lst[idx:idx+cover_hr_in][0]\n",
    "\n",
    "            yield ([h28v05,h28v06,h29v05,h29v06,h31v06,pm_batch],y_batch)\n",
    "            \n",
    "            \n",
    "def pred_generat(tile28v05,tile28v06,tile29v05,tile29v06,remoteweather,pm,batch_size,num_channels,\n",
    "                  num_channels_2,num_channels_3,\n",
    "                  time,time_label,cover_hr_in=1):\n",
    "    while True:\n",
    "        x_shape_1 = (batch_size,time_label,300,300,num_channels)\n",
    "        \n",
    "        x_shape_2 = (batch_size,time_label,300,300,num_channels)\n",
    "        \n",
    "        x_shape_3 = (batch_size,time,4,7,num_channels_2)\n",
    "        \n",
    "        y_shape = (batch_size,time,18)\n",
    "        \n",
    "        \n",
    "        h28v05 = np.zeros(shape=x_shape_1)\n",
    "        h28v06 = np.zeros(shape=x_shape_1)\n",
    "        \n",
    "        h29v05 = np.zeros(shape=x_shape_2)\n",
    "        h29v06 = np.zeros(shape=x_shape_2)\n",
    "        \n",
    "        h31v06 = np.zeros(shape=x_shape_3)\n",
    "        pm_batch = np.zeros(shape=y_shape)\n",
    "        \n",
    "        \n",
    "        #pm = np.zeros(shape=y_shape)\n",
    "        y_batch = np.zeros(shape=y_shape)\n",
    "        \n",
    "        end = pm.shape[0]\n",
    "        #print('this is end ', end)\n",
    "        \n",
    "        \n",
    "        for batch_idx in range(0,end,int(batch_size)):\n",
    "            for seq_idx in range(int(batch_size)):\n",
    "                idx = batch_idx + seq_idx\n",
    "                \n",
    "                h28v05[seq_idx] = tile28v05[idx:idx+cover_hr_in][0]\n",
    "                h28v06[seq_idx] = tile28v06[idx:idx+cover_hr_in][0]\n",
    "                \n",
    "                h29v05[seq_idx] = tile29v05[idx:idx+cover_hr_in][0]\n",
    "                h29v06[seq_idx] = tile29v06[idx:idx+cover_hr_in][0]\n",
    "                \n",
    "                h31v06[seq_idx] = remoteweather[idx:idx+cover_hr_in][0]\n",
    "                pm_batch[seq_idx] = pm[idx:idx+cover_hr_in][0]\n",
    "                #batch_h32v06[seq_idx] = tile32v06[idx:idx+cover_hr_in][0]\n",
    "\n",
    "            \n",
    "            yield ([h28v05,h28v06,h29v05,h29v06,h31v06,pm_batch])\n",
    "            \n",
    "\n",
    "\n",
    "def fn_model_schw3(input_data):\n",
    "    #model_one_conv_1 = Permute((2,1,3,4))(input_data)\n",
    "    model_one_conv_1 = AveragePooling3D(pool_size=(1,2,2), strides=(1,2,2),padding='same')(input_data)\n",
    "    #model_one_conv_1 = AveragePooling3D(pool_size=(1,2,2), strides=(1,2,2),padding='same', data_format='channels_first')(model_one_conv_1)\n",
    "    model_one_conv_1 = TimeDistributed(Conv2D(32, (3,3),strides =(2,2), activation='relu'))(model_one_conv_1)\n",
    "    #model_one_conv_1 = TimeDistributed(AveragePooling2D(pool_size=(2,4), strides=(1,1),padding='same',data_format='channels_first'))(model_one_conv_1)\n",
    "    model_one_conv_1 = TimeDistributed(Conv2D(32, (3,3) ,activation='relu',padding = 'same'))(model_one_conv_1)\n",
    "    \n",
    "\n",
    "    model_one_conv_1 = ConvLSTM2D(32, (3, 3),padding='same', activation='relu',kernel_regularizer=L1L2(l1 = 0.01,l2=0.01),return_sequences=True)(model_one_conv_1)\n",
    "    #model_one_conv_1 = AveragePooling3D(pool_size=(1,3,3), strides=(1,2,2),padding='same',data_format='channels_first')(model_one_conv_1)\n",
    "    model_one_conv_1 = BatchNormalization()(model_one_conv_1)\n",
    "    #model_one_conv_1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2),padding='same')(model_one_conv_1)\n",
    "    model_one_conv_1 = ConvLSTM2D(32, (3, 3), padding='same', activation='relu',kernel_regularizer=L1L2(l1 = 0.01,l2=0.01),return_sequences=True)(model_one_conv_1)\n",
    "    \n",
    "    model_one_conv_1 = AveragePooling3D(pool_size=(1,10,10), strides=(1,10,10),padding='same')(model_one_conv_1)\n",
    "    \n",
    "    \n",
    "    return model_one_conv_1\n",
    "\n",
    "    \n",
    "    \n",
    "def weather_model_schw(input_data):\n",
    "    model_one_conv_1 = ConvLSTM2D(64, (1, 3),padding='same', activation='relu',kernel_regularizer=L1L2(l1 = 0.01,l2=0.01),return_sequences=True)(input_data)\n",
    "    #model_one_conv_1 = TimeDistributed(Conv2D(32, (1,3), activation='relu',data_format='channels_first'))(input_data)\n",
    "    model_one_conv_1 = BatchNormalization()(model_one_conv_1)\n",
    "    #model_one_conv_1 = TimeDistributed(Conv2D(64, (1,3) ,activation='relu',padding = 'same',data_format='channels_first'))(model_one_conv_1)\n",
    "    #model_one_conv_1 = AveragePooling2D(pool_size=(2, 2), strides=(2, 2),padding='same')(model_one_conv_1)\n",
    "    model_one_conv_1 = ConvLSTM2D(42, (1, 3), padding='same', activation='relu',kernel_regularizer=L1L2(l1 = 0.01,l2=0.01), return_sequences=True)(model_one_conv_1)\n",
    "    #model_one_conv_1 = Flatten()(input_data)\n",
    "    #model_one_conv_1 = RepeatVector(time_step)(model_one_conv_1)\n",
    "    #model_one_conv_1 = LSTM(64, activation='relu',dropout = 0.3,recurrent_dropout=0.3,return_sequences=True)(model_one_conv_1)\n",
    "    \n",
    "    return model_one_conv_1\n",
    "    \n",
    "\n",
    "\n",
    "# tile 1\n",
    "data_shape1 = (1,300,300,1)\n",
    "in1 = Input(shape=data_shape1)\n",
    "model1 = fn_model_schw3(in1)\n",
    "\n",
    "# tile 2\n",
    "data_shape2 = (1,300,300,1)\n",
    "in2 = Input(shape=data_shape2)\n",
    "model2 = fn_model_schw3(in2)\n",
    "\n",
    "# tile 3\n",
    "data_shape3 = (1,300,300,1)\n",
    "in3 = Input(shape=data_shape3)\n",
    "model3 = fn_model_schw3(in3)\n",
    "\n",
    "# tile 4\n",
    "data_shape4 = (1,300,300,1)\n",
    "in4 = Input(shape=data_shape4)\n",
    "model4 = fn_model_schw3(in4)\n",
    "\n",
    "#weather model\n",
    "weath_data_shape = (3,4,7,148)\n",
    "in5 = Input(shape=weath_data_shape, name='weath_input')\n",
    "model5 = weather_model_schw(in5)\n",
    "\n",
    "#input for PM2.5 as feature\n",
    "data_shape = (3,18)\n",
    "in6 = Input(shape=data_shape)\n",
    "#model6 = pm25_model(in6)\n",
    "\n",
    "tile1_yr14 = np.transpose(np.load('h28v05_downscale_yr14_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile2_yr14 = np.transpose(np.load('h28v06_downscale_yr14_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile3_yr14 = np.transpose(np.load('h29v05_downscale_yr14_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile4_yr14 = np.transpose(np.load('h29v06_downscale_yr14_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "\n",
    "tile1_yr15 = np.transpose(np.load('h28v05_downscale_yr15_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile2_yr15 = np.transpose(np.load('h28v06_downscale_yr15_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile3_yr15 = np.transpose(np.load('h29v05_downscale_yr15_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile4_yr15 = np.transpose(np.load('h29v06_downscale_yr15_n3day_traindata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "\n",
    "tile1_train = np.concatenate((tile1_yr14,tile1_yr15),axis = 0)\n",
    "tile2_train = np.concatenate((tile2_yr14,tile2_yr15),axis = 0)\n",
    "tile3_train = np.concatenate((tile3_yr14,tile3_yr15),axis = 0)\n",
    "tile4_train = np.concatenate((tile4_yr14,tile4_yr15),axis = 0)\n",
    "\n",
    "weather_yr14 = np.transpose(np.load('remote_weather_yr14_n3day_train.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "weather_yr15 = np.transpose(np.load('remote_weather_yr15_n3day_train.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "\n",
    "remote_weather_train = np.concatenate((weather_yr14,weather_yr15),axis = 0)\n",
    "\n",
    "local_pm = np.load('pm_n3day_daily_train.npy')[:-6].astype('float')\n",
    "tru_val = np.load('pm_label_n3day_daily_train.npy')[:-6].astype('float')\n",
    "\n",
    "\n",
    "tile1_test = np.transpose(np.load('h28v05_downscale_yr16_n3day_testdata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile2_test = np.transpose(np.load('h28v06_downscale_yr16_n3day_testdata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile3_test = np.transpose(np.load('h29v05_downscale_yr16_n3day_testdata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "tile4_test = np.transpose(np.load('h29v06_downscale_yr16_n3day_testdata.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "\n",
    "\n",
    "local_pm_test = np.load('pm_n3day_daily_valid.npy')[:-4].astype('float')\n",
    "tru_val_test = np.load('pm_label_n3day_valid_daily.npy')[:-4].astype('float')\n",
    "remote_weather_test = np.transpose(np.load('remote_weather_n3day_test.npy')[9:].astype('float'),axes = (0,1,3,4,2))\n",
    "\n",
    "train_sample = remote_weather_train[:-6].shape[0]\n",
    "valid_sample = remote_weather_test[:-4].shape[0]\n",
    "\n",
    "chan = 1\n",
    "chan2 = 148\n",
    "chan3 = 4\n",
    "time_t = 3\n",
    "sample_per_epoch = train_sample/sample_batch\n",
    "sample_per_valid = valid_sample/sample_batch\n",
    "\n",
    "\n",
    "print('sample per epoch in train',sample_per_epoch, 'sample per epoch in valid',sample_per_valid)\n",
    "\n",
    "model_final1 = Flatten()(model1)\n",
    "model_final2 = Flatten()(model2)\n",
    "model_final3 = Flatten()(model3)\n",
    "model_final4 = Flatten()(model4)\n",
    "\n",
    "\n",
    "model_final_concat_allAOD = concatenate([model_final1,model_final2,model_final3,model_final4])\n",
    "#model_final_concat_allAOD = concatenate([model_final3,model_final4])\n",
    "\n",
    "\n",
    "#weather_model = TimeDistributed(Flatten())(model5)\n",
    "weather_model = Flatten()(model5)\n",
    "weather_model = RepeatVector(time_step2)(weather_model)\n",
    "\n",
    "model_final_concat = RepeatVector(time_step2)(model_final_concat_allAOD)\n",
    "model_final_concat = concatenate([model_final_concat,weather_model])\n",
    "\n",
    "model_pm = Flatten()(in6)\n",
    "model_pm = RepeatVector(time_step2)(model_pm)\n",
    "\n",
    "#model_final_concat = concatenate([model_final_concat,model6])\n",
    "model_final_concat = concatenate([model_final_concat,model_pm])\n",
    "\n",
    "\n",
    "model_final_concat = TimeDistributed(Dense(64, activation='relu',bias_regularizer=L1L2(l1 = 0.01,l2=0.01)))(model_final_concat)\n",
    "#model_final_concat = Dense(32, activation='relu',bias_regularizer=L1L2(l1 = 0.01,l2=0.01))(model_final_concat)\n",
    "model_final_concat = TimeDistributed(Dropout(rate = 0.4))(model_final_concat)\n",
    "#model_final_concat = Dropout(rate = 0.4)(model_final_concat)\n",
    "model_final_concat = TimeDistributed(Dense(42, activation='relu',bias_regularizer=L1L2(l1 = 0.01,l2=0.01)))(model_final_concat)\n",
    "#model_final_concat = Dense(32, activation='relu',bias_regularizer=L1L2(l1 = 0.01,l2=0.01))(model_final_concat)\n",
    "model_final_concat = TimeDistributed(Dropout(rate=0.4))(model_final_concat)\n",
    "\n",
    "model_final_concat = TimeDistributed(Dense(n_classes))(model_final_concat)\n",
    "\n",
    "model = Model(inputs=[in1,in2,in3,in4,in5,in6],outputs=model_final_concat)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "NB_EPOCH = 3\n",
    "\n",
    "model.compile(optimizer = optim, loss = 'mse', metrics = ['mse'], run_eagerly=True)\n",
    "\n",
    "keras_callback = [\n",
    "                EarlyStopping(monitor='val_mse', patience = 180, mode='min'),\n",
    "                ModelCheckpoint(filepath='next3day_4tile_18stations.h5', monitor='val_mse', \n",
    "                save_best_only=True,mode='min',verbose=1)\n",
    "                ]\n",
    "\n",
    "        \n",
    "history = model.fit(gui_generator(tile1_train[:-6],tile2_train[:-6],tile3_train[:-6],tile4_train[:-6],\n",
    "                                  remote_weather_train[:-6],local_pm,tru_val,sample_batch,chan,\n",
    "                                            chan2,chan3,time_t,time_step),\n",
    "                                              steps_per_epoch = sample_per_epoch,\n",
    "                                            epochs = NB_EPOCH,workers=0,shuffle = False,\n",
    "                              validation_data = gui_generator(tile1_test[:-4],tile2_test[:-4],tile3_test[:-4],tile4_test[:-4],\n",
    "                                            remote_weather_test[:-4],local_pm_test,\n",
    "                                                              tru_val_test,\n",
    "                                            sample_batch,chan,chan2,chan3,time_t,time_step),\n",
    "                                            validation_steps = sample_per_valid,verbose = 1,callbacks=[keras_callback, ClearMemory()])\n",
    "\n",
    "\n",
    "\n",
    "#summarize histoy for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "print('this is val_mse')\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "predictions_edit = model.predict(pred_generat(tile1_test[:-4],tile2_test[:-4],tile3_test[:-4],tile4_test[:-4],\n",
    "                                            remote_weather_test[:-4],local_pm_test,\n",
    "                            sample_batch,chan,chan2,chan3,time_t,time_step),\n",
    "                                 steps = sample_per_valid, verbose=1,workers=0)\n",
    "        \n",
    "\n",
    "predictions = predictions_edit.reshape(predictions_edit.shape[0]*predictions_edit.shape[1],predictions_edit.shape[2])\n",
    "print(predictions_edit.shape)\n",
    "y_label = np.load('pm_label_n3day_valid_daily.npy')[:-4].astype(\"float\")\n",
    "#y_label = np.load('label_valid.npy')\n",
    "y_label2 = y_label.reshape(y_label.shape[0]*y_label.shape[1],y_label.shape[2])\n",
    "print('shape of true and predict ',y_label2.shape,predictions.shape)\n",
    "\n",
    "#perform model evaluation\n",
    "eval_results = eval_metrics_on(predictions,y_label2)\n",
    "print('this is evaluation results for r2,mean_abs_error,mse,rmse,expl_var_score ',eval_results)\n",
    "#print('this is evaluation results for r2,mean_abs_error,mse,rmse,expl_var_score ',eval_results)\n",
    "\n",
    "#observe the graph between prediction and true value of PM2.5 for last hour\n",
    "y_true = y_label[:,2:3,:]\n",
    "y_true = y_true.reshape(y_true.shape[0]*y_true.shape[1],y_true.shape[2])\n",
    "pred_graph = predictions_edit[:,1:2,:]\n",
    "#pred_graph = predictions_edit\n",
    "#pred_graph = predictions_edit.reshape(predictions_edit.shape[0]*predictions_edit.shape[1],predictions_edit.shape[2])\n",
    "pred_graph =pred_graph.reshape(pred_graph.shape[0]*pred_graph.shape[1],pred_graph.shape[2])\n",
    "predict_graph(y_true,pred_graph)\n",
    "\n",
    "print('rmse value')\n",
    "stat_dict = {}\n",
    "stat_avg = []\n",
    "for i in range(time_step2):\n",
    "    t1 = predictions_edit[:,i:i+1,:]\n",
    "    t1 = t1.reshape(t1.shape[0]*t1.shape[1],t1.shape[2])\n",
    "    t2 = y_label[:,i:i+1,:]\n",
    "    t2 = t2.reshape(t2.shape[0]*t2.shape[1],t2.shape[2])\n",
    "    rsl = []\n",
    "    for sn in range(number_stations):\n",
    "        name = 'stats_' + str(sn)\n",
    "        score2 = np.sqrt(np.mean(np.square(t1[:,sn::number_stations]-t2[:,sn::number_stations])))\n",
    "        stat_dict.setdefault(name,[]).append(score2)\n",
    "        #print(score2)\n",
    "        rsl.append(score2)\n",
    "for k in  stat_dict.keys():\n",
    "    avg_value = sum(stat_dict[k])/time_step2\n",
    "    #print(avg_value)\n",
    "    stat_avg.append(avg_value)\n",
    "print('mean of all stations ',mean(stat_avg))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXTRACT REMOTE POLLUTANT FEATURES - Testing feature, then save them to use in stri_p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'concatenate_1'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "predictions_edit = intermediate_layer_model.predict(pred_generat(tile1_test[:-4],tile2_test[:-4],tile3_test[:-4],tile4_test[:-4],\n",
    "                                            remote_weather_test[:-4],local_pm_test,\n",
    "                            sample_batch,chan,chan2,chan3,time_t,time_step),\n",
    "                                 steps = sample_per_valid, verbose=1,workers=0)\n",
    "\n",
    "\n",
    "\n",
    "print(predictions_edit.shape)\n",
    "np.save('middlelayer_test_3day_4tile.npy',predictions_edit)\n",
    "#np.save('prediction14_4hr_ver2.npy',pred_graph)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXTRACT REMOTE POLLUTANT FEATURES - Train feature then save them to use in stri_p model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'concatenate_1'\n",
    "intermediate_layer_model2 = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "predictions_edit = intermediate_layer_model2.predict(pred_generat(tile1_train[:-6],tile2_train[:-6],tile3_train[:-6],\n",
    "                                                                  tile4_train[:-6],\n",
    "                                  remote_weather_train[:-6],local_pm,\n",
    "                            sample_batch,chan,chan2,chan3,time_t,time_step),\n",
    "                                 steps = sample_per_epoch, verbose=1,workers=0)\n",
    "\n",
    "\n",
    "\n",
    "print(predictions_edit.shape)\n",
    "np.save('middlelayer_train_3day_4tile.npy',predictions_edit)\n",
    "#np.save('prediction14_4hr_ver2.npy',pred_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
